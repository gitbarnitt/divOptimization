This reflects files with input from gpt that were checked and checked - supposedly.  It was a peacemeal copy paste becuase I could not download directly nor to googledrive or file IO

✅ So the proper sequence is:
Open the project (.Rproj).

(Optional but recommended) Run:

r
Copy
targets::tar_destroy()
To reset previous results.

Then run:

r
Copy
targets::tar_make(callr_function = NULL)
This will read _targets.R, load your tar_target() definitions, and build the pipeline.

Only use source("_targets.R") if you want to manually check for syntax issues — but this is not required for normal operation.

Let me know when you run tar_make() and what happens!


Move faster with Codex
A cloud-based software engineering agent that answers codebase questions, executes code, and drafts pull requests.

# _targets.R

library(targets)
library(tarchetypes)

# Load custom functions
source("R/load_neon_data.R")
source("R/fit_gjam_model.R")
source("R/fit_gjam_trend_model.R")
source("R/simulate_change.R")
source("R/estimate_detection_prob.R")
source("R/plot_detection_prob.R")
source("R/plot_subsampling.R")
source("R/render_report.R")

# Set package dependencies
tar_option_set(packages = c("dplyr", "tidyr", "gjam", "purrr", "ggplot2", "rmarkdown"))

# Define pipeline
list(
  # Load and split data
  tar_target(neon_data, load_neon_data("data/plant_data.rds")),
  tar_target(site_subsamples, split(neon_data, neon_data$siteID)),

  # Fit year-to-year GJAM models
  tar_target(gjam_fits, lapply(site_subsamples, fit_gjam_model)),

  # Fit trend models
  tar_target(gjam_trend_fits, lapply(site_subsamples, fit_gjam_trend_model)),

  # Simulate detection
  tar_target(simulated_fits, lapply(gjam_fits, simulate_change)),

  # Estimate detection probability
  tar_target(detection_summary, estimate_detection_prob(simulated_fits)),

  # Generate report
  tar_target(
    report,
    render_report(),
    format = "file"
  )
)
